{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9ad92a32-d019-4c9c-85f4-68a97f51dd3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install youtube-transcript-api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7f14ca5e-611f-4b4f-8ede-74d5cd488d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paste youtube url here\n",
    "\n",
    "yt_url = 'https://www.youtube.com/watch?v=_C8kWso4ne4&t=309s'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "9a3ae218-eae6-4d09-9744-d14155e69a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from openai import OpenAI\n",
    "import ollama\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "75ba53af-667e-4a70-8d65-df035ea5e402",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "ollama_api_key = \"http://localhost:11434/api/chat\"\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "open_ai = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "d31ac9eb-d973-4209-bdf9-85ef3dc2e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class youtube_summarizer:\n",
    "\n",
    "    openai_model = \"gpt-4o-mini\"\n",
    "    ollama_model = 'llama3.2'\n",
    "    \n",
    "    system_prompt = \"You are an expert in summarizing the content in the youtube transcript \\\n",
    "                    make the summary presice and cover all the topics in the transcript \\\n",
    "                    do not deviate from the transcript and stick only to the content in the transcript\"\n",
    "    \n",
    "\n",
    "    def __init__(self, url):\n",
    "        self.url = url\n",
    "        text = url.split('?v=')[-1]\n",
    "        self.video_id = text\n",
    "\n",
    "    def get_transcript(self):\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id = self.video_id)\n",
    "        return transcript\n",
    "\n",
    "    def full_transcript(self):\n",
    "        full_transcripts = \"\"\n",
    "        transcript = self.get_transcript()\n",
    "        for i in range(len(transcript)):\n",
    "            full_transcripts += (transcript[i]['text'] + \" \")\n",
    "        return full_transcripts\n",
    "\n",
    "    def messages(self):\n",
    "        messages = [\n",
    "            {'role':'system', 'content':self.system_prompt},\n",
    "            {'role':'user', 'content':self.full_transcript()}\n",
    "        ]\n",
    "        return messages\n",
    "\n",
    "    def openai_summarizer(self):\n",
    "        response = open_ai.chat.completions.create(\n",
    "            model= self.openai_model,\n",
    "            messages= self.messages()\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "        result = display(Markdown(summary))\n",
    "        return result\n",
    "\n",
    "    def ollama_summarizer(self):\n",
    "        response = ollama.chat(model=self.ollama_model, messages=self.messages())\n",
    "        summary = response['message']['content']\n",
    "        result = display(Markdown(summary))\n",
    "        return result\n",
    "        \n",
    "    def ollama_summarizer_through_openai(self):\n",
    "        response = ollama_via_openai.chat.completions.create(\n",
    "            model= self.ollama_model,\n",
    "            messages= self.messages()\n",
    "        )\n",
    "        summary = response.choices[0].message.content\n",
    "        result = display(Markdown(summary))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fe7ac160-de09-4ef3-ad32-1dd02e7fadff",
   "metadata": {},
   "outputs": [],
   "source": [
    "yt_summarizer = youtube_summarizer(yt_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "a23d7f46-248d-4bb0-90aa-54eaeec6620a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The video series introduced PiSpark, an interface for Apache Spark in Python, aimed at large-scale data processing and machine learning. The course, taught by Krish Naik, focuses on using PiSpark with Python, covering topics such as the installation of PiSpark, reading datasets, data preprocessing, and using PiSpark with cloud platforms like Databricks and AWS.\n",
       "\n",
       "Key concepts discussed include:\n",
       "- **Apache Spark's Advantages**: Apache Spark is known for its speed, ease of use, and ability to handle big data through distributed computing.\n",
       "- **Installation and Setup**: Steps to install the PiSpark library using pip and set up a Spark session.\n",
       "- **DataFrames and Operations**: Similarities to pandas, including operations for reading datasets, manipulating data, selecting columns, handling data types, and renaming columns.\n",
       "- **Machine Learning with Spark MLlib**: Introduction to Spark MLlib for performing machine learning tasks, including regression, classification, and clustering.\n",
       "- **Data Preprocessing Techniques**: Handling missing values using methods like dropping, filling with mean/median, and encoding categorical variables.\n",
       "- **Filter Operations**: Techniques for filtering rows based on conditions.\n",
       "- **Group By and Aggregations**: Grouping data and using aggregate functions like sum and mean.\n",
       "\n",
       "The series also covers the usage of Databricks for a cloud-based interface to work with Apache Spark, demonstrating how to create clusters, upload datasets, and run code within Databricks.\n",
       "\n",
       "In summary, the response establishes a comprehensive foundation for working with PiSpark, emphasizing its role in data processing and machine learning while providing practical coding examples and principles. The series promises to delve deeper into machine learning implementations in future videos."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yt_summarizer.openai_summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "adba9451-753c-4fae-8533-b2fefa250590",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here is a step-by-step guide based on the provided script:\n",
       "\n",
       "**Step 1: Load the data**\n",
       "\n",
       "* Import necessary libraries (Python, pandas, numpy)\n",
       "* Load the dataset into a DataFrame using pandas.read_csv() or similar function\n",
       "* Convert data types as needed\n",
       "\n",
       "**Step 2: Split the data into features and target variable**\n",
       "\n",
       "* Identify independent variables (features) and dependent variable (target)\n",
       "* Use pandas.get_dummies() for one-hot encoding if necessary\n",
       "* Separate data into training and testing sets using a library like scikit-learn's train_test_split()\n",
       "\n",
       "**Step 3: Create a linear regression model**\n",
       "\n",
       "* Import necessary libraries (scikit-learn, numpy)\n",
       "* Initialize the model by creating an instance of LinearRegression()\n",
       "* Fit the model to the training data using the fit() method\n",
       "* Get the coefficients and intercept values from the model\n",
       "\n",
       "**Step 4: Evaluate the model**\n",
       "\n",
       "* Use the test set to predict values\n",
       "* Compute performance metrics such as R-squared, mean absolute error, and mean squared error\n",
       "* Print the results\n",
       "\n",
       "**Step 5: Save the model**\n",
       "\n",
       "* Use the `save` method of the model to save it in a pickle format (e.g., `regressor.save('model.pkl')`)\n",
       "* Alternatively, use libraries like joblib or scikit-learn's own serialization functions to save the model\n",
       "\n",
       "Some key concepts and formulas mentioned in the script:\n",
       "\n",
       "* Linear Regression: A statistical method used to predict continuous outcomes\n",
       "* Coefficients and intercept: Parameters of the linear regression equation that capture the relationships between features and the target variable\n",
       "* R-squared (RÂ²): A measure of how well the model fits the data, ranging from 0 to 1\n",
       "* Mean Absolute Error (MAE) and Mean Squared Error (MSE): Measures of the average distance between predicted and actual values\n",
       "\n",
       "Note: This script appears to be written in Python, but the concepts and formulas are applicable to other programming languages as well."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yt_summarizer.ollama_summarizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a5fe9e21-6889-4b48-b5ff-2e74adc8876b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a concise summary of the video:\n",
       "\n",
       "The video demonstrates how to train a multiple linear regression model in Apache Spark using MLlib. The steps involved are:\n",
       "\n",
       "1. Import necessary libraries and define the data sources.\n",
       "2. Load and process the data (no data processing is shown in this video, but it's mentioned elsewhere).\n",
       "3. Split the data into training and testing sets.\n",
       "4. Train a multiple linear regression model using `Regression` from MLlib.\n",
       "5. Get the predictions using `predict` on the test dataset.\n",
       "6. Evaluate the model using performance metrics such as R-squared, mean absolute error (MAE), and mean squared error (MSE).\n",
       "7. Visualize the results.\n",
       "\n",
       "The video also discusses how to save the trained model in a pickle file format and uses it for predictions.\n",
       "\n",
       "Key takeaways:\n",
       "\n",
       "* Define your data sources and load them into Spark.\n",
       "* Split your data into training and testing sets.\n",
       "* Use `Regression` from MLlib to train a multiple linear regression model.\n",
       "* Get predictions using `predict` on the test dataset.\n",
       "* Evaluate your model using performance metrics such as R-squared, MAE, and MSE.\n",
       "\n",
       "Note: The video does not provide explicit code for any of these steps, but it assumes that you have basic knowledge of Apache Spark and MLlib."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "yt_summarizer.ollama_summarizer_through_openai()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041e327-15e0-4fbc-99c2-7919e5868a84",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
